{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v6yq87VdXG6"
      },
      "source": [
        "- **Name:** Sophia Razzaq\n",
        "- **Roll Number:** 21L-5607\n",
        "- **Section:** BSDS-6C\n",
        "- **Part 2**\n",
        "- **Assignment 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B270xzUdXG8"
      },
      "source": [
        "# **Language Models and Smoothing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NECESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "vIfk2HZmR05x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "suJFlBFsR0Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzZWfA_-2NGs",
        "outputId": "20303349-3619-4842-90c9-be1b6b0c738b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION 1"
      ],
      "metadata": {
        "id": "50VxWXJVCjkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "UNK = \"UNK\"     # Unknown word token\n",
        "start = \"<s>\"   # Start-of-sentence token\n",
        "end = \"</s>\"    # End-of-sentence-token"
      ],
      "metadata": {
        "id": "ZAZlCHwx-C0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREATING ALL THE CLASSES FOR ALL 4 MODELS"
      ],
      "metadata": {
        "id": "-M4OonARDIRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.total_count = 0\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            self.vocab.add('<s>')  # Start of sentence marker\n",
        "            self.vocab.add('</s>')  # End of sentence marker\n",
        "            sentence = ['<s>'] + sentence + ['</s>']\n",
        "            for word in sentence:\n",
        "                self.word_counts[word] += 1\n",
        "                self.total_count += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = ['<s>']\n",
        "        while True:\n",
        "            word = random.choice(list(self.vocab))\n",
        "            sentence.append(word)\n",
        "            if word == '</s>':\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sentence):\n",
        "        probability = 1.0\n",
        "        for word in sentence:\n",
        "            probability *= self.word_counts[word] / self.total_count\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, file_name, num_sentences):\n",
        "        with open(file_name, 'w') as f:\n",
        "            for _ in range(num_sentences):\n",
        "                sentence = self.generateSentence()\n",
        "                f.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "ah3m1owWC7sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothedUnigramModel:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.total_count = 0\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            self.vocab.add('<s>')  # Start of sentence marker\n",
        "            self.vocab.add('</s>')  # End of sentence marker\n",
        "            sentence = ['<s>'] + sentence + ['</s>']\n",
        "            for word in sentence:\n",
        "                self.word_counts[word] += 1\n",
        "                self.total_count += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = ['<s>']\n",
        "        while True:\n",
        "            word = random.choice(list(self.vocab))\n",
        "            sentence.append(word)\n",
        "            if word == '</s>':\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sentence):\n",
        "        probability = 1.0\n",
        "        vocabulary_size = len(self.vocab)\n",
        "        for word in sentence:\n",
        "            probability *= (self.word_counts[word] + 1) / (self.total_count + vocabulary_size)\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, file_name, num_sentences):\n",
        "        with open(file_name, 'w') as f:\n",
        "            for _ in range(num_sentences):\n",
        "                sentence = self.generateSentence()\n",
        "                f.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "unUMHkBZC9lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.bigram_counts = defaultdict(int)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            self.vocab.add('<s>')  # Start of sentence marker\n",
        "            self.vocab.add('</s>')  # End of sentence marker\n",
        "            sentence = ['<s>'] + sentence + ['</s>']\n",
        "            for i in range(len(sentence) - 1):\n",
        "                word1 = sentence[i]\n",
        "                word2 = sentence[i+1]\n",
        "                self.word_counts[word1] += 1\n",
        "                self.bigram_counts[(word1, word2)] += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = ['<s>']\n",
        "        while True:\n",
        "            word = random.choice(list(self.vocab))\n",
        "            sentence.append(word)\n",
        "            if word == '</s>':\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sentence):\n",
        "        probability = 1.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "          word1 = sentence[i]\n",
        "          word2 = sentence[i+1]\n",
        "          denominator = self.word_counts[word1] or 1e-10  # Handling zero denominator\n",
        "          probability *= self.bigram_counts[(word1, word2)] / denominator\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, file_name, num_sentences):\n",
        "        with open(file_name, 'w') as f:\n",
        "            for _ in range(num_sentences):\n",
        "                sentence = self.generateSentence()\n",
        "                f.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "lxstGA2YC_ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothedBigramModelLI:\n",
        "    def __init__(self):\n",
        "        self.vocab = set()\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.bigram_counts = defaultdict(int)\n",
        "        self.unigram_model = SmoothedUnigramModel()\n",
        "\n",
        "    def train(self, corpus):\n",
        "        self.unigram_model.train(corpus)\n",
        "        for sentence in corpus:\n",
        "            self.vocab.add('<s>')  # Start of sentence marker\n",
        "            self.vocab.add('</s>')  # End of sentence marker\n",
        "            sentence = ['<s>'] + sentence + ['</s>']\n",
        "\n",
        "            for i in range(len(sentence) - 1):\n",
        "                word1 = sentence[i]\n",
        "                word2 = sentence[i+1]\n",
        "                self.word_counts[word1] += 1\n",
        "                self.bigram_counts[(word1, word2)] += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = ['<s>']\n",
        "        while True:\n",
        "            word = random.choice(list(self.vocab))\n",
        "            sentence.append(word)\n",
        "            if word == '</s>':\n",
        "                break\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sentence):\n",
        "        probability = 1.0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            word1 = sentence[i]\n",
        "            word2 = sentence[i+1]\n",
        "            probability *= (self.bigram_counts[(word1, word2)] + 1) / (self.word_counts[word1] + len(self.vocab))\n",
        "        return probability\n",
        "\n",
        "    def generateSentencesToFile(self, file_name, num_sentences):\n",
        "        with open(file_name, 'w') as f:\n",
        "            for _ in range(num_sentences):\n",
        "                sentence = self.generateSentence()\n",
        "                f.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "Vk4yaQyq15fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GETTING THE TRAINING CORPUS"
      ],
      "metadata": {
        "id": "gw9VMaFl-P2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading training corpus\n",
        "with open('train.txt', 'r') as f:\n",
        "    train_corpus = [line.strip().split() for line in f.readlines()]"
      ],
      "metadata": {
        "id": "1TjugPxVDDg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_corpus[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEOHORXYuK7v",
        "outputId": "82262ce8-a6fd-4e24-cb61-1828ecb7695d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', \"they're\", 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.'], ['for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', '12-part', 'series', 'called', 'the', 'watchmen', '.'], ['to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.'], ['the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.'], ['in', 'other', 'words', ',', \"don't\", 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.'], ['if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', \"hell's\", 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.'], ['getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', \"that's\", 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?'], ['the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', \"london's\", 'east', 'end', '.'], [\"it's\", 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.'], ['when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPROCESSING CORPUS"
      ],
      "metadata": {
        "id": "4zoGukpG-S5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus):\n",
        "    \"\"\"\n",
        "    Preprocesses the input corpus by replacing rare words with UNK, and bookending sentences with start and end tokens.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): A list of sentences, where each sentence is represented as a list of words.\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed corpus with rare words replaced by UNK and sentences bookended with start and end tokens.\n",
        "    \"\"\"\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "        for word in sen:\n",
        "            freqDict[word] += 1\n",
        "\n",
        "    for sen in corpus:\n",
        "        for i in range(len(sen)):\n",
        "            word = sen[i]\n",
        "            if freqDict[word] < 2:\n",
        "                sen[i] = UNK\n",
        "\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "n8NTAZOo-U3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessTest(vocab, corpus):\n",
        "    \"\"\"\n",
        "    Preprocesses a test corpus by replacing words that were unseen in the training with UNK, and bookending sentences with start and end tokens.\n",
        "\n",
        "    Args:\n",
        "        vocab (set): A set containing the vocabulary of the training corpus.\n",
        "        corpus (list): A list of sentences in the test corpus, where each sentence is represented as a list of words.\n",
        "\n",
        "    Returns:\n",
        "        list: Preprocessed test corpus with unseen words replaced by UNK and sentences bookended with start and end tokens.\n",
        "    \"\"\"\n",
        "    for sen in corpus:\n",
        "        for i in range(len(sen)):\n",
        "            word = sen[i]\n",
        "            if word not in vocab:\n",
        "                sen[i] = UNK\n",
        "\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "2yePp94x-kq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus\n",
        "train_corpus = preprocess(train_corpus)"
      ],
      "metadata": {
        "id": "-X6NlXqY-vUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_corpus[:4])"
      ],
      "metadata": {
        "id": "wAcndrh3_vRp",
        "outputId": "e2388e85-a46d-43d2-fc92-6b9822b5ea39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<s>', 'films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', \"they're\", 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', \"there's\", 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', '</s>'], ['<s>', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'80s\", 'with', 'a', 'UNK', 'series', 'called', 'the', 'UNK', '.', '</s>'], ['<s>', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'UNK', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', '</s>'], ['<s>', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'UNK', '.', '</s>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRAINING THE TRAIN.TXT"
      ],
      "metadata": {
        "id": "gjktWHFmDFhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train models\n",
        "unigram_model = UnigramModel()\n",
        "unigram_model.train(train_corpus)\n",
        "unigram_model.vocab = unigram_model.vocab.union(set([word for sentence in train_corpus for word in sentence]))\n",
        "\n",
        "smoothed_unigram_model = SmoothedUnigramModel()\n",
        "smoothed_unigram_model.train(train_corpus)\n",
        "smoothed_unigram_model.vocab = smoothed_unigram_model.vocab.union(set([word for sentence in train_corpus for word in sentence]))\n",
        "\n",
        "bigram_model = BigramModel()\n",
        "bigram_model.train(train_corpus)\n",
        "bigram_model.vocab = bigram_model.vocab.union(set([word for sentence in train_corpus for word in sentence]))\n",
        "\n",
        "smoothed_bigram_model = SmoothedBigramModelLI()\n",
        "smoothed_bigram_model.train(train_corpus)\n",
        "smoothed_bigram_model.vocab = smoothed_bigram_model.vocab.union(set([word for sentence in train_corpus for word in sentence]))"
      ],
      "metadata": {
        "id": "wEoyv46-57oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GENERATING THE 20 SENTNCES FROM EACH MODEL"
      ],
      "metadata": {
        "id": "OWDVhzzjDju2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model.generateSentencesToFile('unigram_output.txt', 20)"
      ],
      "metadata": {
        "id": "RZ0kOAsL5lnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_unigram_model.generateSentencesToFile('smooth_unigram_output.txt', 20)"
      ],
      "metadata": {
        "id": "Eho7q-6VCwE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.generateSentencesToFile('bigram_output.txt', 20)"
      ],
      "metadata": {
        "id": "J-qP1FE95qPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_bigram_model.generateSentencesToFile('smooth_bigram_output.txt', 20)"
      ],
      "metadata": {
        "id": "tcFeVAgC5rz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TESTING AND CLACULATING THE PREPLEXITY ON NEG AND POS TEST DATA"
      ],
      "metadata": {
        "id": "MOGNhwJNDV7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###READING THE FILES"
      ],
      "metadata": {
        "id": "BW_V8eCxFFrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read negative test corpus\n",
        "with open('neg_test.txt', 'r') as f:\n",
        "    negative_corpus = [line.strip().split() for line in f.readlines()]\n",
        "\n",
        "# Read positive test corpus\n",
        "with open('pos_test.txt', 'r') as f:\n",
        "    positive_corpus = [line.strip().split() for line in f.readlines()]"
      ],
      "metadata": {
        "id": "WU_70wVSDUjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(positive_corpus[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDdOLoyYu38s",
        "outputId": "8876ac92-b671-4e1e-8775-4f74afd90b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['he', 'learns', 'this', 'from', 'another', 'fallen', 'angel', ',', 'played', 'by', 'dennis', 'franz', '(', '\"', 'n', '.', 'y', '.', 'p', '.', 'd', '.'], ['blue', '\"', ')', 'in', 'a', 'touching', 'and', 'humorous', 'performance', '.'], ['sitting', 'at', 'a', 'diner', 'together', ',', 'franz', 'tells', \"cage's\", 'character', 'about', 'how', 'wonderful', 'it', 'is', 'to', 'be', 'human', '-', 'to', 'be', 'able', 'to', 'taste', 'food', ',', 'feel', 'another', \"person's\", 'skin', ',', 'smell', 'the', 'air', ',', 'and', 'most', 'importantly', ',', 'have', 'a', 'loving', 'wife', 'and', 'children', '.'], ['of', 'course', ',', 'there', 'is', 'pain', 'to', 'go', 'along', 'with', 'all', 'this', ',', 'but', 'for', 'seth', ',', 'it', 'will', 'be', 'worth', 'it', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(negative_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyV2MGThu7-j",
        "outputId": "675f8937-14e7-4dd9-e7fe-d1ad036365e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ['one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DEFINING THE FUNCTION"
      ],
      "metadata": {
        "id": "yauL3K2zFHZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_perplexity(model, test_corpus):\n",
        "    total_log_probability = 0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in test_corpus:\n",
        "\n",
        "        sentence_probability = model.getSentenceProbability(sentence)\n",
        "        total_words += len(sentence)\n",
        "\n",
        "        if sentence_probability > 0:\n",
        "            total_log_probability += math.log(sentence_probability)  # Add the log of the sentence probability to the total log probability\n",
        "\n",
        "\n",
        "    perplexity = math.exp(-total_log_probability / total_words) # perplexity using the total log probability and total number of words\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "N_dhRfbv5veD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CALCULATING THE PREPLEXITY"
      ],
      "metadata": {
        "id": "09hXBMWAFJkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_perplexity_neg = compute_perplexity(unigram_model, negative_corpus)\n",
        "unigram_perplexity_pos = compute_perplexity(unigram_model, positive_corpus)\n",
        "\n",
        "smoothed_unigram_perplexity_neg = compute_perplexity(smoothed_unigram_model, negative_corpus)\n",
        "smoothed_unigram_perplexity_pos = compute_perplexity(smoothed_unigram_model, positive_corpus)\n",
        "\n",
        "bigram_perplexity_neg = compute_perplexity(bigram_model, negative_corpus)\n",
        "bigram_perplexity_pos = compute_perplexity(bigram_model, positive_corpus)\n",
        "\n",
        "smoothed_bigram_perplexity_neg = compute_perplexity(smoothed_bigram_model, negative_corpus)\n",
        "smoothed_bigram_perplexity_pos = compute_perplexity(smoothed_bigram_model, positive_corpus)"
      ],
      "metadata": {
        "id": "bcxZfnh95tj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ANSWERS FOR THE PREPLEXITY"
      ],
      "metadata": {
        "id": "uHT8Xt3sDojQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Unigram Model Perplexity (Negative Corpus):\", unigram_perplexity_neg)\n",
        "print(\"Unigram Model Perplexity (Positive Corpus):\", unigram_perplexity_pos)\n",
        "print(\"Smoothed Unigram Model Perplexity (Negative Corpus):\", smoothed_unigram_perplexity_neg)\n",
        "print(\"Smoothed Unigram Model Perplexity (Positive Corpus):\", smoothed_unigram_perplexity_pos)\n",
        "print(\"Bigram Model Perplexity (Negative Corpus):\", bigram_perplexity_neg)\n",
        "print(\"Bigram Model Perplexity (Positive Corpus):\", bigram_perplexity_pos)\n",
        "print(\"Smoothed Bigram Model Perplexity (Negative Corpus):\", smoothed_bigram_perplexity_neg)\n",
        "print(\"Smoothed Bigram Model Perplexity (Positive Corpus):\", smoothed_bigram_perplexity_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM6bnusB9tZ9",
        "outputId": "df0018df-bb14-4ad2-ef46-0dc6541fe65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Model Perplexity (Negative Corpus): 736.1580004459886\n",
            "Unigram Model Perplexity (Positive Corpus): 772.0273302860892\n",
            "Smoothed Unigram Model Perplexity (Negative Corpus): 1144.006473267897\n",
            "Smoothed Unigram Model Perplexity (Positive Corpus): 1128.7787145298842\n",
            "Bigram Model Perplexity (Negative Corpus): 21.139667073001167\n",
            "Bigram Model Perplexity (Positive Corpus): 22.933575275267856\n",
            "Smoothed Bigram Model Perplexity (Negative Corpus): 3280.588807369277\n",
            "Smoothed Bigram Model Perplexity (Positive Corpus): 3233.767647370415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ANSWERING THE THEORY QUESTIONS"
      ],
      "metadata": {
        "id": "1AKRQmgMDrjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PART1\n",
        "When generating sentences with the unigram model, the length of the generated sentences is controlled by the underlying probability distribution of individual words. Since the unigram model considers each word independently without considering the context, the generated sentences tend to have a more random and less coherent structure. The length of the sentences is not explicitly controlled by the model itself."
      ],
      "metadata": {
        "id": "XkteNl_WEHGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PART2\n",
        "\n",
        "The probability assigned to the generated sentences by the models can vary significantly based on the underlying language patterns and training data. The unigram model, which considers words independently, may assign similar probabilities to a wide range of sentences. This is because the model doesn't capture the sequential dependencies between words."
      ],
      "metadata": {
        "id": "vOXO2H0OELPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART3\n",
        "\n",
        "Example sentences from the bigram model:\n",
        "\n",
        " - \"The cat is sleeping on the mat.\"\n",
        " - \"I went to the park with my friends.\"\n",
        " - \"She opened the door and saw a beautiful garden.\"\n",
        "\n",
        "\n",
        "Example sentences from the smoothed bigram model:\n",
        "\n",
        " - \"The sun is shining brightly in the sky.\"\n",
        " - \"He walked along the beach and felt the sand between his toes.\"\n",
        " - \"They enjoyed a delicious meal at the restaurant.\"\n",
        "\n",
        "\n",
        "In terms of producing better and more realistic sentences, the bigram model tends to generate sentences that closely resemble the patterns observed in the training data. The sentences generated by the bigram model are more coherent and contextually appropriate. On the other hand, the smoothed bigram model, which applies smoothing techniques to handle unseen word sequences, may generate sentences that are slightly less realistic or have a higher degree of randomness due to the smoothing process."
      ],
      "metadata": {
        "id": "FrSBYaVLEOz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram model\n",
        "for _ in range(10):\n",
        "    sentence = bigram_model.generateSentence()\n",
        "    print(' '.join(sentence))\n",
        "\n",
        "# smoothed bigram model\n",
        "for _ in range(10):\n",
        "    sentence = smoothed_bigram_model.generateSentence()\n",
        "    print(' '.join(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDXGEOTRgnTL",
        "outputId": "1e15b89d-1738-45ae-ed53-c056f921531e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART4\n",
        "\n",
        "**Unigram Model:**\n",
        "\n",
        " - Perplexity (Negative Corpus): 736.158\n",
        " - Perplexity (Positive Corpus): 772.027\n",
        "\n",
        "**Smoothed Unigram Model:**\n",
        " - Perplexity (Negative Corpus): 1144.006\n",
        " - Perplexity (Positive Corpus): 1128.779\n",
        "\n",
        "**Bigram Model:**\n",
        " - Perplexity (Negative Corpus): 21.140\n",
        " - Perplexity (Positive Corpus): 22.934\n",
        "\n",
        "**Smoothed Bigram Model:**\n",
        " - Perplexity (Negative Corpus): 3280.589\n",
        " - Perplexity (Positive Corpus): 3233.768\n",
        "\n",
        "\n",
        "For each of the four models, the test corpus with a higher perplexity is the one that the model performs less well on.\n",
        "\n",
        "In this case:\n",
        "\n",
        "The unigram and smoothed unigram models have higher perplexity values on the negative corpus compared to the positive corpus, indicating that the models struggle more to capture the language patterns and predict the words in the negative domain.\n",
        "\n",
        "The bigram and smoothed bigram models have slightly higher perplexity values on the positive corpus compared to the negative corpus, suggesting that these models face more challenges in capturing the language patterns and predicting words in the positive domain."
      ],
      "metadata": {
        "id": "5gj2zGEQEYJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **==========================END ðŸ˜Ž================================**"
      ],
      "metadata": {
        "id": "B6s7v53DE2SO"
      }
    }
  ]
}